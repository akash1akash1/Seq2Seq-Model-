# -*- coding: utf-8 -*-
"""M22MA010_Assign2_Q1_DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sy5m_BnPzOJhBUTFQCSONIVQNsZki8N5
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
!unzip /content/drive/MyDrive/DL_Assign/Dataset.zip -d /content

!pip install torch==1.8.0 torchtext==0.9.0
#

# !pip install torchtext
from torchtext.legacy.data import Field, BucketIterator,TabularDataset

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import spacy
import random
from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

def tokenize_hi(text):
  return [char for char in text]

def tokenize_en(text):
  return [char for char in text]

hindi = Field(tokenize=tokenize_hi, init_token="<sos>", eos_token="<eos>")
# print(hindi)
english = Field(tokenize=tokenize_en, init_token="<sos>", eos_token="<eos>")



# # Define paths to the data
test_path = '/content/Dakshina Dataset/hi/lexicons/hi.translit.sampled.test.tsv'
val_path = '/content/Dakshina Dataset/hi/lexicons/hi.translit.sampled.dev.tsv'
train_path = '/content/Dakshina Dataset/hi/lexicons/hi.translit.sampled.train.tsv'


train_data, valid_data, test_data = TabularDataset.splits(
path=".", train=train_path,
validation=val_path,
test=train_path, format='tsv',
fields=[('input', english),('output', hindi)]
)

hindi.build_vocab(train_data, max_size=10000, min_freq=2)
print(hindi.vocab.stoi)
english.build_vocab(train_data, max_size=10000, min_freq=2)
print(english.vocab.stoi)

class TextEncoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hid_size, no_layers, dropout):
        super(TextEncoder, self).__init__()
        self.hid_size = hid_size
        self.no_layers = no_layers
        self.dropout = nn.Dropout(dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.LSTM(embed_size, hid_size, no_layers, dropout=dropout)

    def forward(self, input_seq):
        '''# input_seq shape will be (seq_length, batch_size)'''
        embedded = self.dropout(self.embedding(input_seq))
        # embedded shape will be (seq_length, batch_size, embed_size)
        outputs, (hidden, cell) = self.rnn(embedded)
        # outputs shape will be (seq_length, batch_size, hid_size)
        return hidden, cell


class TextDecoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hid_size, output_size, no_layers, dropout):
        super(TextDecoder, self).__init__()
        self.hid_size = hid_size
        self.no_layers = no_layers
        self.dropout = nn.Dropout(dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.LSTM(embed_size, hid_size, no_layers, dropout=dropout)
        self.fc = nn.Linear(hid_size, output_size)

    def forward(self, x, hidden, cell):
        x = x.unsqueeze(0)
        embedded = self.dropout(self.embedding(x))
        outputs, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        predictions = self.fc(outputs)
        predictions = predictions.squeeze(0)
        return predictions, hidden, cell


class Seq2SeqModel(nn.Module):
    def __init__(self, encoder, decoder):
        super(Seq2SeqModel, self).__init__()
        self.decoder = decoder
        self.encoder = encoder

    def forward(self, source, target, teacher_force_ratio=0.5):
        batch_size = source.shape[1]
        target_len = target.shape[0]
        target_vocab_size = len(hindi.vocab)
        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)
        hidden, cell = self.encoder(source)
        x = target[0]

        for t in range(1, target_len):
            output, hidden, cell = self.decoder(x, hidden, cell)
            outputs[t] = output
            best_guess = output.argmax(1)
            x = target[t] if random.random() < teacher_force_ratio else best_guess
        return outputs


### We're ready to define everything we need for training our Seq2Seq model ###

# Training hyperparameters
num_epochs = 10
learning_rate = 0.001
batch_size = 64

# Model hyperparameters
load_model = False
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_size_encoder = len(english.vocab)
print('input_size_encoder',input_size_encoder)
input_size_decoder = len(hindi.vocab)
print('input_size_decoder',input_size_decoder)

output_size = len(hindi.vocab)
print("output_size",output_size)


enc_dropout = 0.5
dec_dropout = 0.5

# Tensorboard to get nice loss plot
writer = SummaryWriter(log_dir="runs/loss_plot")
step = 0

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data),
    batch_size=batch_size,
    sort_within_batch=True,
    sort_key=lambda x: len(x.input),
    device=device,
)

def plot_seq2seq_loss(encoder_embedding_size,decoder_embedding_size,hidden_size,num_layers):
    encoder_net = TextEncoder(
        input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout
    ).to(device)

    decoder_net = TextDecoder(
        input_size_decoder,
        decoder_embedding_size,
        hidden_size,
        output_size,
        num_layers,
        dec_dropout,
    ).to(device)

    model = Seq2SeqModel(encoder_net, decoder_net).to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    pad_idx = english.vocab.stoi["<pad>"]
    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)

    # lstm_output = None

    loss_arr=[]
    accuracy_arr=[]
    for epoch in range(num_epochs):
        running_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        # print(f"[Epoch {epoch+1} / {num_epochs}]")
        model.eval()
        model.train()
        for batch_idx, batch in enumerate(train_iterator):
            # Get input and targets and get to cuda
            inp_data = batch.input.to(device)
            target = batch.output.to(device)
            # Forward prop
            output = model(inp_data, target)
            output = output[1:].reshape(-1, output.shape[2])
            target = target[1:].reshape(-1)
            optimizer.zero_grad()
            loss = criterion(output, target)
            # Back prop
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
            # Gradient descent step
            optimizer.step()
            running_loss += loss.item()

            # Calculate accuracy
            _, predicted = torch.max(output.data, 1)
            total_predictions += target.size(0)
            correct_predictions += (predicted == target).sum().item()

        accuracy = correct_predictions / total_predictions
        print('Epoch no : %d, with CrossEntropyLoss: %.7f, Accuracy: %.7f' % (epoch+1, running_loss/len(train_iterator), accuracy))
        loss_arr.append(running_loss/len(train_iterator))
        accuracy_arr.append(correct_predictions / total_predictions)

      
    import matplotlib.pyplot as plt

    fig, axs = plt.subplots(1, 2, figsize=(12, 6))

    # Plot first plot on first subplot
    axs[0].plot(range(num_epochs), loss_arr)
    axs[0].set_title('Training Loss')
    axs[0].set_xlabel('Epoch')
    axs[0].set_ylabel('Loss')

    # Plot second plot on second subplot
    axs[1].plot(range(num_epochs), accuracy_arr)
    axs[1].set_title('Training Accuracy')
    axs[1].set_xlabel('Epoch')
    axs[1].set_ylabel('Accuracy')

    plt.show()


    # Compute the correlation matrix
    # corr_matrix = np.corrcoef(lstm_output, rowvar=False)
    # print(corr_matrix)



    print("Now checking for Test")
    lstm_output1=None

    test_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_iterator):
            inp_data = batch.input.to(device)
            target = batch.output.to(device)
            output = model(inp_data, target)
            output = output[1:].reshape(-1, output.shape[2])
            if lstm_output1 is None:
                lstm_output1 = output.detach().cpu().numpy()
            else:
                lstm_output1 = np.concatenate((lstm_output1, output.detach().cpu().numpy()), axis=0)
              
            target = target[1:].reshape(-1)
            loss = criterion(output, target)
            test_loss += loss.item()
            
            # Calculate accuracy
            predicted = output.argmax(dim=1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
    test_loss /= len(test_iterator)
    test_acc = 100 * correct / total

    print(f"Test loss: {test_loss:.7f}")
    print(f"Test accuracy: {test_acc:.2f}%")

    return loss_arr,lstm_output1

#for 1st Hyperametres
encoder_embedding_size = 16
decoder_embedding_size = 16
hidden_size = 16  # Needs to be the same for both RNN's
num_layers = 1
loss_arr1,lstm_output1=plot_seq2seq_loss(encoder_embedding_size,decoder_embedding_size,hidden_size,num_layers)

print(lstm_output1)
print(len(lstm_output1))
print(lstm_output1[1])

# import numpy as np
# import seaborn as sns
# import matplotlib.pyplot as plt

# list1 = [1, 2, 3, -4, 5]
# list2 = [6, 7, 8, 10, 10]

# corr = np.corrcoef(list1, list2)

# # Plot the heatmap
# sns.set(font_scale=1.2)
# ax = sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
# ax.set_title('Correlation Matrix')
# plt.show()

print("For 2nd set of Hyperparamters")

encoder_embedding_size = 64
decoder_embedding_size = 64
hidden_size = 64  # Needs to be the same for both RNN's
num_layers = 3
loss_arr2,lstm_output2=plot_seq2seq_loss(encoder_embedding_size,decoder_embedding_size,hidden_size,num_layers)

print(lstm_output2)
print('####################')
print(lstm_output1)
print(len(lstm_output1))
print(lstm_output1[1])
print('####################')

print(lstm_output2[1])

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
   
def corrcoef(word_index):
  # list1 = [1, 2, 3, -4, 5]
  # list2 = [6, 7, 8, 10, 10]
  corr = np.corrcoef(lstm_output1[word_index], lstm_output2[word_index])

  # corr = np.corrcoef(lstm_output1, lstm_output2)
  # Plot the heatmap
  sns.set(font_scale=1.2)
  ax = sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
  ax.set_title('Correlation Matrix')
  plt.show()
corrcoef(1)
corrcoef(5)
corrcoef(7)



plt.plot(range(num_epochs),loss_arr1,label='1st Hyperparametrs')
plt.plot(range(num_epochs),loss_arr2,label='2nd Hyperparametrs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
# Add a legend and title
plt.legend()
plt.title('Plot of loss with different hyperparametrs')

"""**Adding Attenuation in seq2seq model**"""

def tokenize_hi(text):
  return [char for char in text]

def tokenize_en(text):
  return [char for char in text]

hindi = Field(tokenize=tokenize_hi, init_token="<sos>", eos_token="<eos>")
# print(hindi)
english = Field(tokenize=tokenize_en, init_token="<sos>", eos_token="<eos>")



# # Define paths to the data
test_path = '/content/Dakshina Dataset/hi/lexicons/hi.translit.sampled.test.tsv'
val_path = '/content/Dakshina Dataset/hi/lexicons/hi.translit.sampled.dev.tsv'
train_path = '/content/Dakshina Dataset/hi/lexicons/hi.translit.sampled.train.tsv'


train_data, valid_data, test_data = TabularDataset.splits(
path=".", train=train_path,
validation=val_path,
test=train_path, format='tsv',
fields=[('input', english),('output', hindi)]
)

hindi.build_vocab(train_data, max_size=10000, min_freq=2)
print(hindi.vocab.stoi)
english.build_vocab(train_data, max_size=10000, min_freq=2)
print(english.vocab.stoi)

class Encoder_attention(nn.Module):
    def __init__(self, input_size, embed_size, hid_size, no_layers, p):
        super(Encoder_attention, self).__init__()
        self.hid_size = hid_size
        self.embedding = nn.Embedding(input_size, embed_size)
        self.no_layers = no_layers
        self.rnn = nn.LSTM(embed_size, hid_size, no_layers, bidirectional=True)
        self.fc_hidden = nn.Linear(hid_size * 2, hid_size)
        self.fc_cell = nn.Linear(hid_size * 2, hid_size)
        self.dropout = nn.Dropout(p)

    def forward(self, x):
        embedding = self.dropout(self.embedding(x))
        encoder_states, (hidden, cell) = self.rnn(embedding)
        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))
        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))

        return encoder_states, hidden, cell


class Decoder_attention(nn.Module):
    def __init__(
        self, input_size, embed_size, hid_size, output_size, no_layers, p
    ):
        super(Decoder_attention, self).__init__()
        self.hid_size = hid_size
        self.no_layers = no_layers

        self.embedding = nn.Embedding(input_size, embed_size)
        self.rnn = nn.LSTM(hid_size * 2 + embed_size, hid_size, no_layers)

        self.energy = nn.Linear(hid_size * 3, 1)
        self.fc = nn.Linear(hid_size, output_size)
        self.dropout = nn.Dropout(p)
        self.softmax = nn.Softmax(dim=0)
        self.relu = nn.ReLU()

    def forward(self, x, encoder_states, hidden, cell):
        x = x.unsqueeze(0)
        embedding = self.dropout(self.embedding(x))
        sequence_length = encoder_states.shape[0]
        h_reshaped = hidden.repeat(sequence_length, 1, 1)
        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))
        attention = self.softmax(energy)
        context_vector = torch.einsum("snk,snl->knl", attention, encoder_states)
        rnn_input = torch.cat((context_vector, embedding), dim=2)
        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))
        predictions = self.fc(outputs).squeeze(0)
        return predictions, hidden, cell


class Seq2Seq_attention(nn.Module):
    def __init__(self, encoder, decoder):
        super(Seq2Seq_attention, self).__init__()
        self.decoder = decoder
        self.encoder = encoder

    def forward(self, source, target, teacher_force_ratio=0.5):
        batch_size = source.shape[1]
        target_len = target.shape[0]
        target_vocab_size = len(hindi.vocab)

        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)
        encoder_states, hidden, cell = self.encoder(source)

        # First input will be <SOS> token
        x = target[0]

        for t in range(1, target_len):
            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)
            outputs[t] = output
            best_guess = output.argmax(1)
            x = target[t] if random.random() < teacher_force_ratio else best_guess

        return outputs

### We're ready to define everything we need for training our Seq2Seq model ###

# Training hyperparameters
num_epochs = 10
learning_rate = 0.001
batch_size = 64

# Model hyperparameters
load_model = False
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_size_encoder = len(english.vocab)
print('input_size_encoder',input_size_encoder)
input_size_decoder = len(hindi.vocab)
print('input_size_decoder',input_size_decoder)

output_size = len(hindi.vocab)
print("output_size",output_size)

enc_dropout = 0.5
dec_dropout = 0.5

# Tensorboard to get nice loss plot
step = 0

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data),
    batch_size=batch_size,
    sort_within_batch=True,
    sort_key=lambda x: len(x.input),
    device=device,
)


def plot_seq2seq_loss(encoder_embedding_size,decoder_embedding_size,hidden_size,num_layers):


  encoder_net = Encoder_attention(
      input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout
  ).to(device)

  decoder_net = Decoder_attention(
      input_size_decoder,
      decoder_embedding_size,
      hidden_size,
      output_size,
      num_layers,
      dec_dropout,
  ).to(device)

  model = Seq2Seq_attention(encoder_net, decoder_net).to(device)
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)

  pad_idx = english.vocab.stoi["<pad>"]
  criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)

  loss_arr=[]
  accuracy_arr=[]
  for epoch in range(num_epochs):
      running_loss = 0.0
      correct_predictions = 0
      total_predictions = 0
      # print(f"[Epoch {epoch+1} / {num_epochs}]")
      model.eval()
      model.train()
      for batch_idx, batch in enumerate(train_iterator):
          # Get input and targets and get to cuda
          inp_data = batch.input.to(device)
          target = batch.output.to(device)
          # Forward prop
          output = model(inp_data, target)
          output = output[1:].reshape(-1, output.shape[2])
          target = target[1:].reshape(-1)
          optimizer.zero_grad()
          loss = criterion(output, target)
          # Back prop
          loss.backward()
          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
          # Gradient descent step
          optimizer.step()
          running_loss += loss.item()

          # Calculate accuracy
          _, predicted = torch.max(output.data, 1)
          total_predictions += target.size(0)
          correct_predictions += (predicted == target).sum().item()

      accuracy = correct_predictions / total_predictions
      print('Epoch no : %d, with CrossEntropyLoss: %.7f, Accuracy: %.7f' % (epoch+1, running_loss/len(train_iterator), accuracy))
      loss_arr.append(running_loss/len(train_iterator))
      accuracy_arr.append(correct_predictions / total_predictions)

    
  import matplotlib.pyplot as plt

  fig, axs = plt.subplots(1, 2, figsize=(12, 6))

  # Plot first plot on first subplot
  axs[0].plot(range(num_epochs), loss_arr)
  axs[0].set_title('Training Loss')
  axs[0].set_xlabel('Epoch')
  axs[0].set_ylabel('Loss')

  # Plot second plot on second subplot
  axs[1].plot(range(num_epochs), accuracy_arr)
  axs[1].set_title('Training Accuracy')
  axs[1].set_xlabel('Epoch')
  axs[1].set_ylabel('Accuracy')

  plt.show()



  print("Now checking for Test")
  lstm_output1=None
  test_loss = 0.0
  correct = 0
  total = 0
  with torch.no_grad():
      for batch_idx, batch in enumerate(test_iterator):
          inp_data = batch.input.to(device)
          target = batch.output.to(device)
          output = model(inp_data, target)
          output = output[1:].reshape(-1, output.shape[2])
          if lstm_output1 is None:
              lstm_output1 = output.detach().cpu().numpy()
          else:
              lstm_output1 = np.concatenate((lstm_output1, output.detach().cpu().numpy()), axis=0)
            
          target = target[1:].reshape(-1)
          loss = criterion(output, target)
          test_loss += loss.item()
          
          # Calculate accuracy
          predicted = output.argmax(dim=1)
          total += target.size(0)
          correct += (predicted == target).sum().item()
          
  test_loss /= len(test_iterator)
  test_acc = 100 * correct / total

  print(f"Test loss: {test_loss:.7f}")
  print(f"Test accuracy: {test_acc:.2f}%")




  return loss_arr,lstm_output1


  # return loss_arr

encoder_embedding_size = 16
decoder_embedding_size = 16
hidden_size = 16  # Needs to be the same for both RNN's
num_layers = 1
loss_arr1,lstm_output1=plot_seq2seq_loss(encoder_embedding_size,decoder_embedding_size,hidden_size,num_layers)

print("For 2nd set of Hyperparamters")

encoder_embedding_size = 64
decoder_embedding_size = 64
hidden_size = 64  # Needs to be the same for both RNN's
num_layers = 1
loss_arr2,lstm_output2=plot_seq2seq_loss(encoder_embedding_size,decoder_embedding_size,hidden_size,num_layers)

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

def corrcoef(word_index):
  # list1 = [1, 2, 3, -4, 5]
  # list2 = [6, 7, 8, 10, 10]
  corr = np.corrcoef(lstm_output1[word_index], lstm_output2[word_index])

  # corr = np.corrcoef(lstm_output1, lstm_output2)
  # Plot the heatmap
  sns.set(font_scale=1.2)
  ax = sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
  ax.set_title('Correlation Matrix')
  plt.show()
corrcoef(1)
corrcoef(5)
corrcoef(7)

plt.plot(range(num_epochs),loss_arr1,label='1st Hyperparametrs')
plt.plot(range(num_epochs),loss_arr2,label='2nd Hyperparametrs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
# Add a legend and title
plt.legend()
plt.title('Plot of loss with different hyperparametrs')